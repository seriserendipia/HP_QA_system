{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4b9vZN+L50VGMezXhLf2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seriserendipia/HP_QA_system/blob/main/myHarry_Potter_QA_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P068yfnpsz2q",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "rwRav48KImOO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hugging_face_key = userdata.get('Hugging_Face_Token')\n",
        "deepseek_key = userdata.get('DeepSeek_Token')\n",
        "from huggingface_hub import login\n",
        "login(token=hugging_face_key)"
      ],
      "metadata": {
        "id": "h6VrWe2mFljV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "# Initialize embedding model\n",
        "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "prompt = None\n",
        "\n",
        "# Load and process the Harry Potter text\n",
        "with open(\"/content/01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Split text into chunks\n",
        "def split_into_chunks(text, max_length=300):\n",
        "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "chunks = split_into_chunks(raw_text)\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "chunk_embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
        "\n",
        "# Build FAISS index\n",
        "index = faiss.IndexFlatL2(chunk_embeddings[0].shape[0])\n",
        "index.add(np.array(chunk_embeddings))\n",
        "\n",
        "# Initialize Hugging Face QA model\n",
        "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=deepseek_key, base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "# Function to retrieve context based on the question\n",
        "def get_context(question, top_k=5):\n",
        "    q_embed = embedder.encode([question])[0]\n",
        "    _, I = index.search(np.array([q_embed]), top_k)\n",
        "    context = \"\\n\".join([chunks[i] for i in I[0]])\n",
        "    return context\n",
        "\n",
        "# Function to get answer from Hugging Face model\n",
        "def ask_hf(prompt):\n",
        "    result = qa_pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
        "    return result.strip()\n",
        "\n",
        "# Function to get answer from DeepSeek model\n",
        "def ask_deepseek(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-chat\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        stream=False\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Combined function to get answers from both models\n",
        "def ask_both(question):\n",
        "    context = get_context(question)\n",
        "    prompt = f\"Answer the question based on the context.\\nContext: {context}\\nQuestion: {question}\"\n",
        "    print(\"prompt:\", prompt)\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logging.info(prompt)\n",
        "\n",
        "    hf_answer = ask_hf(prompt)\n",
        "    ds_answer = ask_deepseek(prompt)\n",
        "    return hf_answer, ds_answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q5zzYBnQ1tn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_both(\"How do you make something float in the air?\")"
      ],
      "metadata": {
        "id": "mENQ3EwA8Sd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ðŸ§™ Harry Potter Knowledge QA System\")\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Your Magic Question\")\n",
        "        hf_output = gr.Textbox(label=\"ðŸ§  Hugging Face Answer\")\n",
        "        ds_output = gr.Textbox(label=\"ðŸ”® DeepSeek Answer\")\n",
        "\n",
        "    question_input.submit(ask_both, inputs=question_input, outputs=[hf_output, ds_output])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "Y1eAwfLA1i1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txfB0PP03I75"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}